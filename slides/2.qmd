---
title: "Multiple Linear Regression"
format: beamer
editor: visual
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
```

# Univariate Case: Simple Linear Regression

-   Is there a linear relation between biking and heart disease?

```{r}
library(tidyverse)
library(knitr)
d1 <- read_csv('heart_data.csv')
d = d1
kable(head(d1, 10) %>% select(-smoking))
```

# Simple Linear Regression

-   Regress heart_disease on biking

-   Response/Dependent Variable: heart_disease

-   Predictor variable: biking

# 

```{r}
l2 <- lm(heart_disease ~ biking, data = d1)

d1 = d1 %>%
    bind_cols(
        pred2 = predict(l2, data = d1)
    ) 

rss2 = sum((d1$heart_disease-d1$pred2)^2)

p1 = d1 %>%
    ggplot(aes(x = biking)) +
    geom_point(aes(y = heart_disease), color = "red")

p2 = d1 %>%
    ggplot(aes(x = biking)) +
    geom_point(aes(y = heart_disease), color = "red") +
    geom_abline(intercept = coef(l2)[1], slope = coef(l2)[2],
                color = "blue", linewidth = 1)

p3 = d1 %>%
    ggplot(aes(x = biking))+
    geom_point(aes(y = heart_disease), color = "red") +
    geom_abline(intercept = coef(l2)[1], slope = coef(l2)[2],
                color = "blue", linewidth = 1)+
    geom_text(x = min(d1$biking)+.1, y = max(d1$heart_disease) - .1, label = paste0("R2 = ", round(summary(l2)$r.squared, 2)))
print(p3)
```

# 

```{r}
library(jtools)
summary(l2)
```

# Example Data

```{r}
library(tidyverse)
library(knitr)
d1 <- read_csv('heart_data.csv')
d = d1
kable(head(d1, 15))
```

# Univeriate approach: Simple Linear Model

-   Regress heart_disease on biking

```{r}
library(tidyverse)
library(ggplot2)


l1 <- lm(heart_disease ~ biking, data = d1)

d1 = d1 %>%
    bind_cols(
        pred1 = predict(l1, data = d1)
    )

rss1 = sum((d1$heart_disease-d1$pred1)^2)

p1 = d1 %>%
    ggplot(aes(x = biking)) +
    geom_point(aes(y = heart_disease), color = "red")

p3 = d1 %>%
    ggplot(aes(x = biking)) +
    geom_point(aes(y = heart_disease), color = "red") +
    geom_abline(intercept = coef(l1)[1], slope = coef(l1)[2],
                color = "blue", linewidth = 1)+
    geom_text(x = min(d1$biking)+.1, y = max(d1$heart_disease) - .1, label = paste0("R2 = ", round(summary(l1)$r.squared, 2)))
    
print(p3)
```

# 

```{r}
library(jtools)
summary(l2)
```

# Univeriate approach: Simple Linear Model

-   Regress heart_disease on smoking

```{r}

l2 <- lm(heart_disease ~ smoking, data = d1)

d1 = d1 %>%
    bind_cols(
        pred2 = predict(l2, data = d1)
    ) 

rss2 = sum((d1$heart_disease-d1$pred2)^2)

p1 = d1 %>%
    ggplot(aes(x = smoking)) +
    geom_point(aes(y = heart_disease), color = "red")

p2 = d1 %>%
    ggplot(aes(x = smoking)) +
    geom_point(aes(y = heart_disease), color = "red") +
    geom_abline(intercept = coef(l2)[1], slope = coef(l2)[2],
                color = "blue", linewidth = 1)

p3 = d1 %>%
    ggplot(aes(x = smoking))+
    geom_point(aes(y = heart_disease), color = "red") +
    geom_abline(intercept = coef(l2)[1], slope = coef(l2)[2],
                color = "blue", linewidth = 1)+
    geom_text(x = min(d1$smoking)+.1, y = max(d1$heart_disease) - .1, label = paste0("R2 = ", round(summary(l2)$r.squared, 2)))
print(p3)
```

# 

```{r}
library(jtools)
summary(l2)
```

# 

-   Is there a better way? better model?

# Multivariate Approach: Multiple Regression Model

-   heart_disease = $\beta_0$ + $\beta_1\cdot$ biking + $\beta_2\cdot$ smoking + $\epsilon$

-   $\epsilon \sim N(0, \sigma^2)$

# Graphing the solution

```{r}
library("plot3D")

x <- d1$smoking
y <- d1$biking/7
z <- d1$heart_disease

# Compute the linear regression 
fit <- lm(z ~ x + y)

d1 = d1 %>%
    bind_cols(
        pred3 = predict(fit, data = d1)
    ) 

rss3 = sum((d1$heart_disease-d1$pred3)^2)

# create a grid from the x and y values (min to max) and predict values for every point
# this will become the regression plane
grid.lines = 40
x.pred <- seq(min(x), max(x), length.out = grid.lines)
y.pred <- seq(min(y), max(y), length.out = grid.lines)
xy <- expand.grid( x = x.pred, y = y.pred)
z.pred <- matrix(predict(fit, newdata = xy), 
                 nrow = grid.lines, ncol = grid.lines)

# create the fitted points for droplines to the surface
fitpoints <- predict(fit)

# scatter plot with regression plane
scatter3D(x, y, z, pch = 19, cex = 1,colvar = NULL, col="red", 
          theta = 20, phi = 10, bty="b",
          xlab = "smoking", ylab = "biking", zlab = "heart_disease",  
          surf = list(x = x.pred, y = y.pred, z = z.pred,  
                      facets = TRUE, fit = fitpoints, col=ramp.col (col = c("dodgerblue3","seagreen2"), n = 300, alpha=0.9), border="black"), main = paste0("RSS: ",round(rss3,2), ", R2 = ", round(summary(fit)$r.squared, 2)))
```

-   heart_disease = 14.98 + -0.2 $\cdot$ biking + 0.18 $\cdot$ smoking

# 

```{r}
library(jtools)
summary(fit)
```

# Model Definition

$$
y = \beta_0 + \beta_1x_1 + + \beta_2x_2 +...+ \beta_px_p + \epsilon
$$

-   Model Assumptions

    -   (A1) The response variable $y$ is a random variable and the predictor $x_1, x_2, ..., x_n$ is non-random

    -   (A2) $\epsilon \sim N(0, \sigma^2)$

# 

```{=tex}
\begin{center}
\Huge Parameters Estimation
\end{center}
```
# Data Presentation

![](9.png)

# Matrix Equation of MLR

![](10.png)

# 

```{=tex}
\begin{center}
\Huge Goodness of Fit
\end{center}
```
# Coefficient of Determination

-   Similarly to the case of SLR, we have

![](2.png)

-   And $$
    R^2 = \frac{RegSS}{TSS}  = 1-\frac{RSS}{TSS}
    $$

# F-test

-   Full Model:

$$
y = \beta_0 + \beta_1x_1 + + \beta_2x_2 +...+ \beta_px_p + \epsilon
$$

-   Baseline Model or i.i.d model:

$$
y = \beta_0  + \epsilon
$$

-   The baseline model is equivalent to

$$
\beta_{1} = \beta_{2} =...= \beta_{p} = 0
$$

# 

-   We would like to test for the joint significant of all predictors, or if the full model is a significant improvement over the baseline model, or

![](14.png)

# 

-   Test Statistics

![](11.png)

# ANOVA Table

-   The results of MLR are usually summarized in the ANOVA table

![](17.png)

# Example

An actuary uses multiple regression model with three predictors and 20 observations and has the following results.

|            | Sum of Squares |
|------------|----------------|
| Regression | 150            |
| Total      | 200            |

He wants to test the following hypothesis

$H_0: \beta_1 = \beta_2 = \beta_3 = 0$

$H_1:$ At least one of $\beta_1$, $\beta_2$, and $\beta_3$ is zero

Calculate the F-statistics of the test.

# 

# From R2 to F-test

-   The $R^2$ and the $F-statistics$ have the following relation

$$
F = \frac{RegSS/p}{RSS/(n-p-1)} = \frac{R^2/p}{(1-R^2)/(n-p-1)}
$$ and

$$
R^2 = \frac{Fp}{Fp+n-p-1}
$$

# Example

Sarah performs a regression of the return on a mutual fund ($y$) on four predictors plus an intercept. She uses monthly returns over 105 months. Her software calculates the $R^2 = .8$ but then it quits working before it calculates the value of $F$. Calculates the F-statistics for Sarah.

# 

# Generalized F-test

-   Full Model: $$
    y = \beta_0 + \beta_1x_1 + + \beta_2x_2 +...+ \beta_px_p + \epsilon
    $$

-   Reduced Model: $$
    y = \beta_0 + \beta_1x_1 + + \beta_2x_2 +...+ \beta_{p-q}x_{p-q} + \epsilon
    $$

# 

![](12.png)

# 

| Model   | $RSS$   | $RegSS$   |       |
|---------|---------|-----------|-------|
| Reduced | $RSS_0$ | $RegSS_0$ | $TSS$ |
| Full    | $RSS_1$ | $RegSS_1$ | $TSS$ |

# 

-   $H_0: \beta_{p-q+1} = \beta_{p-q+2} =...= \beta_{p-q} = 0$ or Reduced model is adequate

-   Test Statistics

![](13.png)

# Example

![](15.png)

# 

# Example

You wish to find a model to predict insurance sales, using 27 observations and 8 variables $x_1$, $x_2$,...,$x_8$. The analysis of variance (ANOVA) tables are below. Model A contains all 8 variables and Model B contains $x_1$ and $x_2$ only.

Calculate the F-statistics for testing $H_0: \beta_3 = \beta_4 = \beta_5 = \beta_6 = \beta_7 = \beta_8 = 0$

# 

![](16.png)

# 
