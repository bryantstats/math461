
---
title: "Hierarchical Clustering in R"
author: <font size="5"> Son Nguyen </font>
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>`
---

<style>

.remark-slide-content {
  background-color: #FFFFFF;
  border-top: 80px solid #F9C389;
  font-size: 17px;
  font-weight: 300;
  line-height: 1.5;
  padding: 1em 2em 1em 2em
}

.inverse {
  background-color: #696767;
  border-top: 80px solid #696767;
  text-shadow: none;
  background-image: url(https://github.com/goodekat/presentations/blob/master/2019-isugg-gganimate-spooky/figures/spider.png?raw=true);
	background-position: 50% 75%;
  background-size: 150px;
}

.your-turn{
  background-color: #8C7E95;
  border-top: 80px solid #F9C389;
  text-shadow: none;
  background-image: url(https://github.com/goodekat/presentations/blob/master/2019-isugg-gganimate-spooky/figures/spider.png?raw=true);
	background-position: 95% 90%;
  background-size: 75px;
}

.title-slide {
  background-color: #F9C389;
  border-top: 80px solid #F9C389;
  background-image: none;
}

.title-slide > h1  {
  color: #111111;
  font-size: 40px;
  text-shadow: none;
  font-weight: 400;
  text-align: left;
  margin-left: 15px;
  padding-top: 80px;
}
.title-slide > h2  {
  margin-top: -25px;
  padding-bottom: -20px;
  color: #111111;
  text-shadow: none;
  font-weight: 300;
  font-size: 35px;
  text-align: left;
  margin-left: 15px;
}
.title-slide > h3  {
  color: #111111;
  text-shadow: none;
  font-weight: 300;
  font-size: 25px;
  text-align: left;
  margin-left: 15px;
  margin-bottom: -30px;
}

</style>

```{css, echo=FALSE}
.left-code {
  color: #777;
  width: 48%;
  height: 92%;
  float: left;
}
.right-plot {
  width: 51%;
  float: right;
  padding-left: 1%;
}
```

```{r setup, include = FALSE}
# R markdown options
knitr::opts_chunk$set(echo = TRUE, 
                      
                      fig.width = 10,
                      fig.height = 5,
                      fig.align = "center", 
                      message = FALSE,
                      warning = FALSE)

# Load packages
library(tidyverse)
```
# Dataset

We study the dataet `USArests`, which comes with R. This data set contains statistics, in arrests per 100,000 residents for assault, murder, and rape in each of the 50 US states in 1973. Also given is the percent of the population living in urban areas. A row in the dataset presents a state.

We will use the package `factoextra`. Use the below code to install the package.

```{r, eval=FALSE}
install.packages('factoextra')
```
---
# Import and scale the data

```{r}
library(tidyverse)
library(factoextra)

df <- USArrests
# remove missing values
df <- na.omit(df)
# scale the data for clustering
df <- scale(df)
```

---
# Visualize the distances

For a small dataset, we can actually visual the distances between all the data point using the distance matrix.

```{r}
distance <- get_dist(df)
fviz_dist(distance, gradient = list(low = "red", mid = "green", high = "white"))
```

From the heatmap of the distance matrix, we have an idea of which observations are close (red color) or far away (green color) from each other.

---
# Decide the number of clusters

Plot the total within sum of squares of the data for different numbers of clusters.

```{r}
fviz_nbclust(df, FUN = hcut, method = "wss")
```

We look for the `elbow` point of the graph, to identify the number of cluster. Looking at the graph, we can argue that the `elbow` point of this graph is at the number of cluster being 4. 


---
# Hierarchical Clustering

```{r}
d <- dist(df, method = "euclidean")

# method could be "single", "complete", "average", "median", or "centroid"
hi_clustering <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hi_clustering)
```

---
# Dendogram.

```{r}
# Visualize the cluster
plot(hi_clustering, cex = 0.6)
rect.hclust(hi_clustering, k = 4, border = 2:5)
```


---
# Clustering and PCA

```{r}
# Assign clusters to the observations
sub_grp <- cutree(hi_clustering, k = 4)
df = data.frame(df)
df = df %>%
  mutate(cluster = sub_grp)
fviz_cluster(list(data = df, cluster = sub_grp))
```

Notice that the horizontal axis is the first principal component and the vertical axis is the second principal component analysis. We can see that the two principals break down the clusters quite clearly.

---
# Practice

Working with the [Maill Customers](Mall_Customers2.csv) dataset.

-   Read the dataset using `read_csv` and scale the dataset using `scale`.

-   Perform Hierarchical Clustering on the data

-   Plot the total sum squares within clusters and use the `elbow` method to decide the number of clusters.

-   Visualize (with dendogram and the principal components) the clusters with the selected number of clusters.

```{r, eval=FALSE, echo=FALSE}
library(tidyverse)
library(factoextra)

df <- read_csv('Mall_Customers2.csv')

#scale each variable to have a mean of 0 and sd of 1
df <- as_tibble(scale(df))

# Dissimilarity matrix
d <- dist(df, method = "euclidean")

# Hierarchical clustering using Complete Linkage
# Method could be single, average...

hi_clustering <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hi_clustering)

fviz_nbclust(df, FUN = hcut, method = "wss")


sub_grp <- cutree(hi_clustering, k = 3)

df = df %>%
  mutate(cluster = sub_grp)

plot(hi_clustering, cex = 0.6)
rect.hclust(hi_clustering, k = 3, border = 2:5)

fviz_cluster(list(data = df, cluster = sub_grp))
```

