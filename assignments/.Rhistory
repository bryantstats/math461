set.seed(2023)
n = 50
y = rep(0, n)
y[1] = 0
b0 = 0
b1 = 1.1
e = rnorm(n, sd = 1)
for (t in 2:n)
{
y[t] = b0 + b1*y[t-1]+e[t]
}
y = ts(y)
autoplot(y)
set.seed(2023)
n = 50
y = rep(0, n)
y[1] = 0
b0 = 0
b1 = -.2
e = rnorm(n, sd = 1)
for (t in 2:n)
{
y[t] = b0 + b1*y[t-1]+e[t]
}
y = ts(y)
autoplot(y)
y = arima.sim(list(order = c(1,0,0), ar = .1))
yt <- arima.sim(list(order=c(1,0,0), ar=-.6), n=100)
b0 = 10
yt <- yt + b0
acf(yt)
acf(y)
set.seed(2024)
yt <- arima.sim(list(order=c(1,0,0), ar=-.6), n=100)
b0 = 10
yt <- yt + b0
acf(yt)
acf(y)
set.seed(2024)
yt <- arima.sim(list(order=c(1,0,0), ar=-.8), n=100)
b0 = 10
yt <- yt + b0
acf(yt)
acf(y)
set.seed(2024)
yt <- arima.sim(list(order=c(1,0,0), ar=.8), n=100)
b0 = 10
yt <- yt + b0
acf(yt)
acf(y)
set.seed(2024)
yt <- arima.sim(list(order=c(1,0,0), ar=-.9), n=100)
b0 = 10
yt <- yt + b0
acf(yt)
acf(y)
set.seed(2024)
yt <- arima.sim(list(order=c(1,0,0), ar=.2), n=100)
b0 = 10
yt <- yt + b0
acf(yt)
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=.2), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=-.2), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=-.8), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=.7), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=.5), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=.3), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=-.5), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=-.9), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=.0), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=.1), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=1), n=100)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar= .9), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=.1), n=100)
b0 = 10
y <- y + b0
acf(y)
yt <- arima.sim(list(order=c(1,0,0), ar=.5), n=1000)
b0 = 10
yt <- yt + b0
acf(yt)
yt <- arima.sim(list(order=c(1,0,0), ar=.9), n=1000)
b0 = 10
yt <- yt + b0
acf(yt)
yt <- arima.sim(list(order=c(1,0,0), ar=-.6), n=1000)
b0 = 10
yt <- yt + b0
acf(yt)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar=-.5), n=100)
b0 = 10
y <- y + b0
acf(y)
set.seed(2024)
y <- arima.sim(list(order=c(1,0,0), ar= -.9), n=100)
b0 = 10
y <- y + b0
acf(y)
yt <- arima.sim(list(order=c(1,0,0), ar=-.9), n=1000)
b0 = 10
yt <- yt + b0
acf(yt)
yt <- arima.sim(list(order=c(1,0,0), ar=-.99), n=1000)
b0 = 10
yt <- yt + b0
acf(yt)
yt <- arima.sim(list(order=c(1,0,0), ar=-.9), n=1000)
b0 = 10
yt <- yt + b0
acf(yt)
library(knitr)
y =  c(14, 5, 11, 4, 11, 3)
t = length(y)
y_bar = mean(y)
u = y[-t]-y_bar
v = (y-y_bar)[2:t]
b1 = sum(u*v)/sum(u^2)
b0 = y_bar*(1-b1)
df = data.frame(t = c(1:6), y = y)
kable(df)
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, eval = FALSE)
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaint.csv")
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints.csv")
df
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df
as_tibble(df, .name_repair = "minimal")
as_tibble(df, .name_repair = )
?as_tibble
as_tibble(df, .name_repair = "universal" )
names(df)
df = as_tibble(df, .name_repair = "universal" )
names(df)
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
# create the DTM
df_tm <- df %>%
unnest_tokens(output = word, input = Consumer.complaint.narrative) %>%
anti_join(get_stopwords()) %>%
anti_join(tibble(word = c(letters, LETTERS, "oh", 'just', 've', as.character(c(1:100)))))
word_freq <- df_tm %>%
group_by(document) %>% count(word, sort = TRUE)
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
library(wordcloud)
df %>%
unnest_tokens(input = text, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
library(wordcloud)
df %>%
unnest_tokens(input = Consumer.complaint.narrative, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
stop_word2 = tibble(word = c(letters, LETTERS, "oh", 'just'))
library(wordcloud)
df %>%
unnest_tokens(input = Consumer.complaint.narrative, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
stop_word2 = tibble(word = c(letters, LETTERS, "oh", 'just', "XXXX", "XX"))
library(wordcloud)
df %>%
unnest_tokens(input = Consumer.complaint.narrative, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
stop_word2 = tibble(word = c(letters, LETTERS, "oh", 'just', "XXXX", "XX"))
df %>%
unnest_tokens(input = text, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
stop_word2 = tibble(word = c(letters, LETTERS, "oh", 'just', "XXXX", "XX"))
df %>%
unnest_tokens(input = Consumer.complaint.narrative, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
library(wordcloud)
df %>%
unnest_tokens(input = Consumer.complaint.narrative, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
stop_word2 = tibble(word = c(letters, LETTERS, "oh", 'just', "XXXXX", "XX"))
df %>%
unnest_tokens(input = Consumer.complaint.narrative, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
library(wordcloud)
df %>%
unnest_tokens(input = Consumer.complaint.narrative, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
df = df %>% rename(texts = Consumer.complaint.narrative)
stop_word2 = tibble(word = c(letters, LETTERS, "oh", 'just', "XXXXX", "XX"))
df %>%
unnest_tokens(input = texts, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
library(wordcloud)
df %>%
unnest_tokens(input = texts, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
# Sentiment Analysis
df %>%
unnest_tokens(input = text, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2) %>%
inner_join(get_sentiments("nrc")) %>%
filter(!is.na(sentiment)) %>%
count(sentiment, sort = TRUE) %>%
ggplot(aes(sentiment, n))+geom_col()+
labs(y='Relative Frequency', x ='')
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
df = df %>% rename(texts = Consumer.complaint.narrative)
stop_word2 = tibble(word = c(letters, LETTERS, "oh", 'just', "XXXXX", "XXXX", "XX"))
df %>%
unnest_tokens(input = texts, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
library(wordcloud)
df %>%
unnest_tokens(input = texts, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
library(tidyverse)
library(tidytext)
library(tm)
library(wordcloud)
df <- read_csv("complaints2.csv")
df = as_tibble(df, .name_repair = "universal" )
df = df %>% rename(texts = Consumer.complaint.narrative)
stop_word2 = tibble(word = c(letters, LETTERS, "oh", 'just', "XXXXX", "XXXX", "XX", "xxxx", "xxxxx", "xx"))
df %>%
unnest_tokens(input = texts, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
head(10) %>%
ggplot(aes(x = n, y = reorder(word, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
library(wordcloud)
df %>%
unnest_tokens(input = texts, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2)%>%
count(word, sort = TRUE)%>%
with(wordcloud(word, n, random.order = FALSE,
max.words = 50, colors=brewer.pal(8,"Dark2")))
# Sentiment Analysis
df %>%
unnest_tokens(input = text, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2) %>%
inner_join(get_sentiments("nrc")) %>%
filter(!is.na(sentiment)) %>%
count(sentiment, sort = TRUE) %>%
ggplot(aes(sentiment, n))+geom_col()+
labs(y='Relative Frequency', x ='')
# Sentiment Analysis
df %>%
unnest_tokens(input = texts, output = word) %>%
anti_join(get_stopwords()) %>%
anti_join(stop_word2) %>%
inner_join(get_sentiments("nrc")) %>%
filter(!is.na(sentiment)) %>%
count(sentiment, sort = TRUE) %>%
ggplot(aes(sentiment, n))+geom_col()+
labs(y='Relative Frequency', x ='')
df_bigrams <- df %>%
unnest_tokens(input = texts, output = bigram, token = "ngrams", n = 2) %>%
filter(!is.na(bigram))
df_bigrams %>%
count(bigram, sort = TRUE)
df_bigrams %>%
count(bigram, sort = TRUE) %>%
head(10) %>%
ggplot(aes(x = n, y = reorder(bigram, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
library(igraph)
bigram_graph <- bigram_counts %>%
filter(n > 50) %>%
graph_from_data_frame()
bigrams_separated <- df_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% stop_words2) %>%
filter(!word2 %in% stop_words2)
stop_words2
stop_words2 = tibble(word = c(letters, LETTERS, "oh", 'just', "XXXXX", "XXXX", "XX", "xxxx", "xxxxx", "xx"))
bigrams_separated <- df_bigrams %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word1 %in% stop_words2) %>%
filter(!word2 %in% stop_words2)
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigram_counts
bigram_counts$pairs = paste0(bigram_counts$word1, " ", bigram_counts$word2)
bigram_counts %>%
head(10) %>%
ggplot(aes(x = n, y = reorder(pairs, n))) +
geom_col() +
labs(y = '', x = 'Frequency')
# R markdown options
knitr::opts_chunk$set(echo = FALSE,
fig.width = 10,
fig.height = 5,
fig.align = "center",
message = FALSE,
warning = FALSE)
# Load packages
library(tidyverse)
library(forecast)
library(knitr)
library(tidyverse)
library(lubridate)
library(readr)
library(TSstudio)
library(tidyverse)
library(lubridate)
library(readr)
# Load the AirPassengers dataset
data("AirPassengers")
ts_data <- AirPassengers
# Convert the time series to a data frame
ts_df <- data.frame(Date = index(ts_data), Passengers = coredata(ts_data))
library(tidyverse)
library(lubridate)
library(readr)
library(xts)
# Load the AirPassengers dataset
data("AirPassengers")
ts_data <- AirPassengers
# Convert the time series to a data frame
ts_df <- data.frame(Date = index(ts_data), Passengers = coredata(ts_data))
# Convert Date to a time series object
ts_df$Date <- as.Date(ts_df$Date)
ts_xts <- xts(ts_df$Passengers, order.by = ts_df$Date)
# Create lag features for time series data
lags <- 1:12  # Number of lags to consider
lagged_data <- lag(ts_xts, k = lags)  # Create lagged data
